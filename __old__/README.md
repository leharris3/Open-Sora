# Text2Bricks: Fine-tune Open-Sora For Generating Brick Style Animations.

<div align="center">
    <a href="https://api.wandb.ai/links/lambdalabs/d71480sb"><img src="https://img.shields.io/badge/wandb-report-purple"></a>
    <a href="https://huggingface.co/datasets/lambdalabs/text2bricks"><img src="https://img.shields.io/badge/Huggingface-datasets-purple"></a>
    <a href="https://huggingface.co/lambdalabs/text2bricks-360p-64f"><img src="https://img.shields.io/badge/Huggingface-model-purple"></a>
    <a href="http://207.211.177.211:7860/"><img src="https://img.shields.io/badge/gradio-demo-purple"></a>    
</div>

Welcome to the tutorial for this brick video project! This project is about the make of a customized [Open-Sora](https://github.com/hpcaitech/Open-Sora) model that generates brick style stop animations from text inputs. 

__You can play with the videos created by the models in this [game](https://albrick-hitchblock.s3.amazonaws.com/index.html).__

Below are some examples  generated by the model, trained as described in this tutorial.
<div align="center">
  <table>
    <tr>
      <td><img src="./assets/demo/text2bricks_32f_sample_15.gif" width="480" height="270" alt="A young man walks alone by the seaside."/></td>
      <td width="240">A young man walks alone by the seaside.</td>
    </tr>
    <tr>
      <td><img src="./assets/demo/text2bricks_32f_sample_8.gif" width="480" height="270" alt="A ninja wearing a red outfit jumps from one roof of a building to a second building's roof. The full moon is in sight directly behind the ninja."/></td>
      <td width="240">A ninja wearing a red outfit jumps from one roof of a building to a second building's roof. The full moon is in sight directly behind the ninja.</td>
    </tr>
    <tr>
      <td><img src="./assets/demo/text2bricks_32f_sample_5.gif" width="480" height="270" alt="A shot of the sunset from a beautiful beach with white sand and crashing waves. No people or animals in sight. Few clouds that are lit up in orange and red from the sunset. The Sun is halfway set."/></td>
      <td width="240">A shot of the sunset from a beautiful beach with white sand and crashing waves. No people or animals in sight. Few clouds that are lit up in orange and red from the sunset. The Sun is halfway set.</td>
    </tr>
  </table>
</div>


This repository is a fork of [hpcaitech/Open-Sora](https://github.com/hpcaitech/Open-Sora).
Highlights include:
- Our customized models ([32frame](https://huggingface.co/lambdalabs/text2bricks-360p-32f) and [64frame](https://huggingface.co/lambdalabs/text2bricks-360p-64f))
- Instructions to reproduce fine-tuning on our [text2bricks dataset](https://huggingface.co/datasets/lambdalabs/text2bricks).
- Learning Rate Scheduling that uses Cosine Warmup and annealing.
- [Wandb report](https://api.wandb.ai/links/lambdalabs/d71480sb) of validation outputs

This guide will walk you through the steps needed to install the necessary dependencies, prepare the dataset, train the model, and run inferences on Lambda's [1-Click Cluster](http://lambdalabs.com/service/gpu-cloud/1-click-clusters). Let's get started!

## Installation / Setup
To get started with this project, follow these installation steps:
1. Make sure miniconda is installed by testing the following command
    ```bash
    conda init
    ```
2. **Download the Installer**
   ```bash
   wget https://raw.githubusercontent.com/LambdaLabsML/Open-Sora/lambda_bricks/install.sh
   yes | bash install.sh
   ```
3. **Verify the Installation**:
    The following command should print out `SUCCESS` if your environment is set up correctly.
    ```bash
    cd Open-Sora
    python install-check.py
    ```
4. **Activate Environment**:
    Activate the environment before any of the actions listed below.
    ```bash
    conda activate "brick-osora"
    ```



## Dataset Preparation
Here is a toy dataset (1000 clips):
1. Download `bricks.zip` [here](https://lambdaml.s3.us-west-1.amazonaws.com/brick.zip) and unzip it's content into the `Open-Sora` folder.
    ```bash
    unzip bricks.zip
    unzip brick/brick_clips.zip
    ```
    This should have unzipped 1000 mp4 files into `./brick_clips`.
2. Verify that the paths match the demo-csv file for this dataset named `Open-Sora/brick_clips.csv` and are accessible from within the `Open-Sora` folder.

The full dataset is available on [Huggingface](https://huggingface.co/datasets/lambdalabs/text2bricks). To prepare your own dataset:
1. Setup a Python virtual envrionment using the following script.
    ```bash
    ./scripts/env_setup_data_process.sh
    ```
2. Prepare a `video_urls.txt` file of list of YouTube video links, and run the following data process script.
   ```bash
    python -m scripts.data_process \
    --output /path/where/new/dataset/will/be/created \
    --url-file /path/to/video_urls.txt  \
    --video-dir /path/to/some/videos \
    --prompt video-lego \
    --caption gpt4o \ 
    --num-p 8 \
    --key [OPENAI_API_KEY]
   ```


## Train the Model

We released two models and shared their details in this [Weights & Biases report](https://api.wandb.ai/links/lambdalabs/d71480sb).

This is the command to train [lambdalabs/text2bricks-360p-32f](https://huggingface.co/lambdalabs/text2bricks-360p-32f):

```
OMP_NUM_THREADS=52 colossalai run --nproc_per_node 8 \
--hostfile $HOSTFILE \
--master_addr $MASTER_ADDR \
scripts/train.py \
configs/opensora-v1-1/train/text2bricks-360p-32f.py \
--data-path /path/where/dataset/csv/is/located \
--ckpt-path /path/to/OpenSora-STDiT-v2-stage3/model.safetensors
```

This is the command to train [lambdalabs/text2bricks-360p-64f](https://huggingface.co/lambdalabs/text2bricks-360p-64f):

```
OMP_NUM_THREADS=52 colossalai run --nproc_per_node 8 \
--hostfile $HOSTFILE \
--master_addr $MASTER_ADDR \
scripts/train.py \
configs/opensora-v1-1/train/text2bricks-360p-64f.py \
--data-path /path/where/dataset/csv/is/located \
--ckpt-path /path/to/OpenSora-STDiT-v2-stage3/model.safetensors
```


## Running Inference

```
python scripts/inference.py \
configs/opensora-v1-1/inference/text2bricks-360p-32f.py \
--prompt "A young man walks alone by the seaside." \
--num-frames 32

python scripts/inference.py \
configs/opensora-v1-1/inference/text2bricks-360p-64f.py \
--prompt "A young man walks alone by the seaside." \
--num-frames 64
```

---

Feel free to reach out if you have any questions or encounter any issues. Happy coding!
